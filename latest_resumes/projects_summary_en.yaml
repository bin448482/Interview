schema_version: 1.1
generated_at: '2025-11-28T10:40:46.354834'
source_file: Resume_Data_Engineer_CN_20250529.md
projects:
- project_name: MySixth Tarot Card Intelligent Application
  company_or_context: Zoetis / Individual Independent Developer
  timeframe:
    label: October 2025 - Present
    start: 2025-10
    end: null
  role_title: Independent developer
  role_perspective: hybrid
  management_scope:
    team_size: 1
    budget_level: lt_100k
    stakeholder_tiers:
    - customer
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - hands_on_build
  - commercial_strategy
  responsibility_focus:
  - architecture
  - implementation
  - commercialization
  impact_metrics:
    business_metrics:
    - Paid conversion rate reaches 1.5 times the industry average.
    technical_metrics:
    - Unified LLM factory and caching strategy reduce AI call costs by 60%.
    - Two-phase AI interpretation pipeline stably supports personalized divination
      content
    operational_metrics:
    - The monthly operating cost of a single cloud server deployment will be kept
      below 500 yuan.
    - One-time delivery of Android/iOS/Web/Admin four-end suite, accelerate version
      iteration
  governance_artifacts:
  - runbook
  project_overview: 'TarotAI Omnichannel Kit: Expo React Native client + FastAPI backend
    + Next.js admin panel + AI content generation tool, offering anonymous users a
    four-step Tarot experience and paid AI interpretations. GitHub: https://github.com/bin448482/tarotAI'
  data_domain: Consumer-level Tarot Fortune Telling / AI Content Operation
  ai_component_flag: true
  challenges_or_objectives:
  - Under a single team, simultaneously deliver mobile, backend, admin panel, and
    AI-generated tools.
  - Ensure anonymous identity, offline synchronization, and multi-channel payment
    coordination under the same architecture.
  - Build a two-phase AI pipeline and multi-LLM routing while controlling inference
    costs.
  - Achieve Docker-based delivery, balancing Google Play, redemption codes, and commercial
    strategies like Stripe.
  responsibilities:
  - 'Leading four-end architecture: Expo mobile, FastAPI service, Next.js Admin, AI
    mass generation tool'
  - Design anonymous installation ID + JWT authentication, four-step divination process,
    and end-to-end API contract.
  - Implement the recharge route and risk control for Google Play IAP/coupon codes/Stripe
    (reserved)
  - Set up Docker Compose + Nginx deployment, monitoring, and offline data backup
    strategy.
  - Create multilingual README/CLAUDE guidelines, standardize development process
  architecture_or_solution:
  - Expo RN (SDK54) client + FastAPI monolithic + Next.js 15 Admin + Python AI Generator
    layered architecture
  - 'Dual-stage AI API: `/readings/analyze` for recommended dimensions, `/readings/generate`
    for paid interpretations'
  - 'Anonymous Identity System: `installation_id` + optional email binding + JWT access
    control'
  - 'Offline Priority: Pre-set in Expo SQLite `tarot_config.db` with sync strategy,
    background SQLite independent persistence'
  - 'Payment and Channels: Google Play Billing, Bulk Code Generation, Stripe Checkout
    Reserve Interface'
  - Docker Compose (backend/admin/nginx) + Nginx routes `/api/*` to backend, others
    to admin.
  - 'AI Content Operation: `tarot-ai-generator` Typer CLI connects SQLite with GLM-4/OpenAI/Ollama
    to achieve batch dimension copywriting.'
  process_or_methodology:
  - Vibe Coding / Modular Component System + Expo Router Four-step Process
  - TypeScript/Python unified coding standards, ESLint/Pylint review, Zod/Pydantic
    type guard
  - Offline sync + SQLite version control + Docker volume backup strategy
  - EAS Build + CI/CD + Multi-environment `.env` management
  - Document-driven development (README/CLAUDE) and single-bay multi-application management
  deliverables_or_features:
  - 'Expo mobile app `my-tarot-app`: Three-card spread/Celtic Cross, AI-paid interpretation,
    history records, icon script'
  - 'FastAPI `tarot-backend`: LLM Gateway, Payment, Admin Interface, Static Assets,
    Health Check'
  - 'Next.js `tarot-admin-web`: Dashboard, User/Point Operation, Bulk Code Generation,
    Order Source Tracking, Client Download Portal'
  - 'Tarot AI Generator: Multi-mode batch generation, multi-language routing, JSON
    import/export scripts'
  - Nginx + Docker Compose unified deployment topology, offline SQLite volume backup
    manual
  - 'Recharge Strategy: IAP priority → Redemption code/Stripe fallback, compatible
    with anonymous identity'
  metrics_or_impact:
  - A production-level suite that runs synchronously on Android/iOS/Web/Admin.
  - Two-stage AI interpretation expands static card meaning into personalized narratives,
    supporting paid conversion.
  - Anonymous identity + offline caching enables four-step divination even in unregistered/weak
    network scenarios.
  - Docker-based deployment shortens environment setup time, SQLite volume backup
    process reduces O&M risks.
  tech_stack:
  - Expo React Native (SDK 54 / React Native 0.81)
  - TypeScript 5.x
  - Next.js 15 App Router + Ant Design 6 + Tailwind CSS
  - FastAPI 0.104 + SQLAlchemy + Uvicorn
  - SQLite (Expo/Backend)
  - Python 3.10 + Typer CLI
  - GLM-4 (ZhiPuAI)
  - OpenAI API
  - Ollama/Claude (removable)
  - LangChain / Self-developed LLM Router
  - Docker & Docker Compose
  - Nginx Proxy
  - Google Play Billing / Stripe Checkout (Reserved)
  tools_platforms:
  - Expo Router 6
  - Status + SWR
  - EAS Construction
  - Ant Design Charts
  - ESLint / Pylint / Prettier
  - CI/CD Pipeline + generate-icons.js script
  - Docker volume backup solution
  team_info:
    team_size: 1
    description: Personal independent development + AI collaboration + multi-warehouse
      document-driven
  notes: 'Detailed design in tarotAI repository (Expo client, FastAPI backend, Next.js
    admin panel, AI generator, Docker/Nginx deployment). GitHub: https://github.com/bin448482/tarotAI'
- project_name: NGSE (Next Generation Sales Engine)
  company_or_context: Zoetis
  timeframe:
    label: 2023.6 - 2025.9
    start: 2023-06
    end: 2025-09
  role_title: Project Manager & Data Architect & AI Application Developer
  role_perspective: hybrid
  management_scope:
    team_size: 5
    budget_level: gt_1m
    stakeholder_tiers:
    - exec
    - director
    - ops
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - people_management
  - risk_governance
  responsibility_focus:
  - planning
  - architecture
  - stakeholder_management
  impact_metrics:
    business_metrics:
    - The project has become a regional digital benchmark case and has received continuous
      investment.
    technical_metrics:
    - Key report execution time improved by 2~3 times.
    - Data accuracy improved to 99.5% to support AI training
    operational_metrics:
    - Establish corporate-level data warehouse development standards and promote them
      to other markets.
  governance_artifacts:
  - exec_dashboard
  - risk_register
  project_overview: Leading the development of NGSE data platform architecture in
    China, independently built a Databricks lakehouse system to support AI training,
    business analysis, and multi-market data processing.
  data_domain: Global Sales Engine / Data Platform
  ai_component_flag: true
  challenges_or_objectives:
  - 0→1 Establish a standardized data platform
  - Ensure reliable data processing across multiple markets and tasks.
  - Balancing batch processing rollback, incremental update, and task scheduling.
  responsibilities:
  - Design data pipeline architecture and integrate with Azure ecosystem.
  - Develop data quality, monitoring, and performance optimization strategies.
  - Build an integrated Lakehouse and multi-market reusable Notebook template
  - Manage ADF scheduling and cross-platform data transmission
  architecture_or_solution:
  - 'Four-tier data processing architecture: Inbox→Raw→Transform→Governed'
  - Databricks Workflows + Delta Lake ensure dependency management and ACID compliance.
  - Incremental strategy for automatically verifying T-1 data discrepancies
  - Spark SQL/PySpark Common ETL Module and Method Registration Mechanism
  - Parameterized Notebook联动ADF, enabling multi-market reuse.
  process_or_methodology:
  - Lake and warehouse integration best practices
  - Modularized and atomized data development specifications
  - Automated Data Quality Verification and Anomaly Alert
  deliverables_or_features:
  - Inter-platform data transmission link (Aliyun OSS→ADLS Gen2)
  - Schema Drift/Field Integrity/Null Value Duplicate Monitoring Module
  - Partitioning of hotspots, Z-Ordering, caching, and dynamic aggregation
  metrics_or_impact:
  - Key report execution time increased by 2~3 times.
  - Establish company-level data warehouse development specifications
  - Enhance data reliability and support AI training needs.
  tech_stack:
  - Databricks
  - Delta Lake
  - Azure Data Factory
  - Azure Storage
  - Spark SQL
  - PySpark
  - Azure Functions
  tools_platforms:
  - Databricks Workflows
  - ADF stands for "Abstract Data Format" or "Application Development Framework."
  - Azure Data Lake Storage Gen2
  team_info:
    team_size: null
    description: null
  notes: The text describes its leadership in China's regional platform architecture
    without specifying a specific job title.
- project_name: AI-enhanced intelligent resume submission system
  company_or_context: Individual independent developer
  timeframe:
    label: 2025.1 - 2025.9
    start: 2025-01
    end: 2025-09
  role_title: Independent developer
  role_perspective: developer
  management_scope:
    team_size: 1
    budget_level: lt_100k
    stakeholder_tiers: []
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - hands_on_build
  responsibility_focus:
  - architecture
  - implementation
  - operations
  impact_metrics:
    business_metrics:
    - Recommendation satisfaction increased by 25%, new job discovery rate increased
      by 40%.
    technical_metrics:
    - RAG processing throughput >50 positions/min with a matching accuracy of 91.27%.
    - Vector retrieval latency <100ms and memory usage maintained <4GB
    operational_metrics:
    - Position extraction speed > 100 positions/min, system availability > 99%
  governance_artifacts:
  - runbook
  project_overview: The MyThird platform driven by Python and LangChain integrates
    job extraction, RAG analysis, intelligent matching, LangChain Agent, Selenium
    automated submission, and anti-scraping behavior simulation into a scriptable
    job-hunting automation suite.
  data_domain: Job Application/Automation + RAG Intelligent Analysis
  ai_component_flag: true
  challenges_or_objectives:
  - Under manual login, achieve end-to-end delivery automation on sites like Zhipu,
    51Job, and Boss, while avoiding risk control.
  - 'Build an extensible multi-layer architecture: CLI/Agent entry → Control layer
    → Processing layer → Data layer'
  - Simultaneously provides online/offline vector models and multi-LLM routing, maintaining
    \>50 positions/minute RAG throughput
  - Establish an interpretable multi-dimensional scoring and decision-making engine
    to enhance the accuracy of Chinese semantic matching.
  responsibilities:
  - Design the control layer for MasterController + JobScheduler + DecisionEngine
    + DataBridge, responsible for task scheduling, priority, and fault tolerance.
  - Implement LangChain RAG pipeline, ChromaDB/SQLite dual storage, Redis caching,
    and sentence-transformers embedding management.
  - Developed GenericResumeJobMatcher/MultiDimensionalScorer/SmartMatchingEngine and
    scoring reports, HTML output
  - Realize ResumeSubmissionEngine, ButtonRecognition, JobStatusDetector, AntiCrawlerSystem,
    BehaviorSimulator, covering multi-site artificial behavior simulation.
  - Develop rag_cli.py, integration_main.py, batch tool, YAML configuration system,
    offline model script, and pytest/verify_* tests.
  architecture_or_solution:
  - 'User Interaction: CLI, LangChain Agent chat, Future Web Console → Unified MasterController'
  - 'Control Layer: MasterController + JobScheduler + DecisionEngine + Monitoring
    + ErrorHandler'
  - 'Processing Layer: Position Extraction, RAG Pipeline, Intelligent Matching, Automatic
    Submission, Analysis Engine (asyncio + Typer CLI)'
  - 'Data Layer: SQLite (position/matching/application status), ChromaDB (semantic
    vectors), Redis Cache, Log/Report Directory'
  - 'AI Stack: LangChain + Zhipu GLM-4-Flash + sentence-transformers (offline, supports
    m3e/text2vec) + Pluggable OpenAI/Claude/Ollama'
  - 'Automation: Selenium WebDriver + BehaviorSimulator + AntiCrawlerSystem (random
    delays, mouse tracking, session maintenance, user agent rotation)'
  - 'Pipeline: Position extraction → RAG vectorization → Matching scoring → Decision
    filtering (salary/competitiveness) → Delivery execution → Status update/report'
  process_or_methodology:
  - Typer CLI + YAML configuration driver (rag_cli, integration_main, scripts/setup_local_models.py)
  - 'Asynchronous/concurrent: asyncio + priority scheduling + checkpoint/retry + idle
    mode'
  - LangChain Agent tooling + STRUCTURED_CHAT style prompt + RAG toolkit
  - Multi-document CLAUDE.md architecture document + module细分指南 + performance/monitoring
    checklist
  - pytest, verify_integration.py, test_master_controller.py, run_all_rag_tests.py
    - Full-stack Tests
  deliverables_or_features:
  - 'rag_cli: status/search/pipeline/matching/resume processing/chat commands set'
  - 'LangChain Agent: Job Market Q&A, Salary Analysis, Trend Insights'
  - MasterController end-to-end pipeline + DecisionEngine decision threshold and salary
    filtering + JobScheduler concurrency control
  - GenericResumeJobMatcher + Multi-dimensional Scoring + Time-aware Matching + Personalized
    Recommendations with HTML Report
  - Resume Submission Engine + Job Status Detector + Button Recognition + Anti-Crawler
    System + Behavior Simulator
  - Offline model management script (setup_local_models.py / download_models.py) and
    data validation tools (check_database_structure.py, verify_database.py)
  - 'Batch Processing and Recovery Tools: batch_rematch_jobs.py, pipeline_results/,
    reports/'
  metrics_or_impact:
  - Position extraction speed >100 positions/min; RAG processing >50 positions/min;
    matching processing >200 results/min; system availability >99%
  - Vector retrieval latency <100ms, memory usage <4GB/instance
  - 'Multi-dimensional scoring accuracy: Skill matching rate 88.5%, skill matches
    23/26, semantic similarity 0.6-0.8, Chinese matching accuracy +30%.'
  - 'Comprehensive matching accuracy 91.27% (Five-dimensional rating: semantics/skills/experience/industry/salary)'
  - Time perception strategies increase new position discovery rate by +40% and recommendation
    satisfaction by +25%.
  tech_stack:
  - Python 3.8+ / AsyncIO / Typer CLI
  - LangChain / Structured Chat Agents
  - Intellifly GLM-4-Flash / OpenAI / Claude / Ollama (removable)
  - sentence-transformers (text2vec, m3e, MiniLM, mpnet, etc.)
  - ChromaDB Vector Database
  - SQLite (jobs.db + resume profiles)
  - Redis Cache
  - Selenium WebDriver + Behavior Simulator
  - FastAPI/Flask auxiliary service (internal tool/API)
  - pytest / unittest suite
  - YAML configuration + Pydantic/Dataclasses
  tools_platforms:
  - rag_cli.py / rag_system admin console
  - integration_main.py (MasterController entry)
  - LangChain Agent CLI
  - Generic Resume Job Matcher & Smart Matching Engine
  - MasterController / JobScheduler / DecisionEngine / DataBridge
  - Resume Submission Engine / Anti-Crawler System / Behavior Simulator
  - ChromaDB + SQLite + Redis + pipeline_results/metrics monitoring
  - 'scripts/setup_local_models.py / download_models.py / verify_database.py

    Translation:

    scripts/setup_local_models.py / download_models.py / verify_database.py'
  team_info:
    team_size: 1
    description: Individual independent development, covering AI, RAG, automation,
      and anti-crawling systems.
  notes: For details, see D:/0-development/projects/MyThird (MyThird.code-workspace
    + Modular CLAUDE document architecture).
- project_name: AI-enhanced Image Crawler experiment
  company_or_context: Individual independent developer
  timeframe:
    label: 2024.6 - 2025.9
    start: 2024-06
    end: 2025-09
  role_title: Independent developer
  role_perspective: developer
  management_scope:
    team_size: 1
    budget_level: lt_100k
    stakeholder_tiers: []
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - hands_on_build
  responsibility_focus:
  - architecture
  - implementation
  - operations
  impact_metrics:
    business_metrics: []
    technical_metrics:
    - ResNet validation accuracy 95.71%, false negative rate down to 3.85%.
    - YOLO-Pose detection accuracy >90%, classification accuracy >96%
    operational_metrics:
    - Image extraction success rate and ad filtering accuracy both reach 100%.
    - Web console tracks 895+ records/2,731 images in real-time, with zero duplicates
      and misjudgments.
  governance_artifacts:
  - runbook
  - playbook
  project_overview: 'MyFirst Image Crawler Platform: Custom forum crawler +
    SHA-256 deduplication + SQLite data lake + Flask web console + ResNet/YOLO-Pose
    classification and attention heatmaps, achieving the closed-loop of collection
    → governance → prediction → operation.'
  data_domain: Vertical image acquisition / content governance / computer vision
  ai_component_flag: true
  challenges_or_objectives:
  - Adapts to non-standard `ess-data`/`data-link` attributes on target pages, ensuring
    100% image capture
  - Advertising filtering (20% page content) and SHA-256 deduplication completed in
    bulk collection, zero false positives
  - Unified SQLite + Web console, real-time monitoring of 895+ crawling records (245.8
    MB) and 2,731 images (including AI/Manual interest tags)
  - Using ResNet/CBAM + YOLO-Pose regional perception model, improve Chinese adult
    image recall, false negatives reduced from 21.54% to 3.85%.
  - Provide batch reasoning, attention heatmaps, Fine-tuning/YOLO guidelines, and
    support for continuous model evolution.
  responsibilities:
  - Design YAML configuration, crawler CLI entrypoints, ad blacklist, and
    date-based storage to ensure reliable crawling.
  - Implement SHA-256 duplicate detection, `crawl_results` schema, content hash
    naming, automatic backup and migration scripts
  - Developed Flask + WebSocket admin panel (`start_web_app.py`), fixed default filter
    values, implemented image gallery, select all/batch marking, and SQL console.
  - Build ResNet18/CBAM/adaptive Focal Loss training pipeline, YOLO-Pose guided region-aware
    classifier, batch prediction/attention heatmap tool
  - Write fine_tuning_*, CLAUDE.md, yolo_guided_* documents and test/validation tools
    such as run_tests.py, check_database.py.
  architecture_or_solution:
  - 'Crawler: config/examples/image_crawler_fixed.yaml defines selector/ad_filtering;
    run_image_crawler.py, collect_interested_images.py, batch_analyze_pages.py'
  - 'Storage: SQLite + SQLAlchemy + `image_storage` + `simple_duplicate_checker`; fields
    include content_hash, is_interested, ml_prediction_confidence; `data/images/YYYY-MM-DD/`
    directory with hash naming'
  - 'Web: Flask blueprint + `/api/dashboard/stats`, `/api/images`, `/api/query/execute`,
    image gallery, real-time monitoring, batch operations, configuration editing'
  - ML directory contains dataset, model_factory, inference, scripts, ResNet & BBoxAttention
    & YOLO-Pose models; Adaptive Focal Loss, DynamicSampler, multi-level randomization,
    progressive unfreezing
  - 'Pipeline: Scrape → Ad filter → SHA-256 deduplication → SQLite storage → Web review
    → ResNet/YOLO inference → Interest tags/heatmaps → Batch export/retraining'
  process_or_methodology:
  - Configuration driver (YAML), Typer/Click CLI, content-hash naming, log and system
    table monitoring
  - 'Automated testing: run_tests.py, tests/unit/test_duplicate_detection.py, verify_database.py,
    evaluate_fine_tuning.py'
  - 'ML Engineering: Adaptive Focal Loss, DynamicSampler, Multi-level Randomization,
    YOLO-Pose完整性测试 (8/8 Passed)'
  - 'Fine-tuning Process: SmartColorAugmentation, DynamicSampleBalancer, Noise Injection,
    Gradual Unfreezing, OverfittingMonitor'
  - 'Document-driven: CLAUDE.md, fine_tuning_summary.md, fine_tuning_usage_guide.md,
    yolo_guided_* guidelines record each repair.'
  deliverables_or_features:
  - Dedicated crawler script for target forum, ad filtering, content hash deduplication, date
    partitioning, batch page analysis
  - 'Flask Web Console: Dashboard, SQL Query, Image Gallery, Select/Batch Operations,
    Real-time Monitoring, Configuration Editing'
  - ResNet/CBAM classification model, batch_predict_bbox_attention.py, image_classifier_with_visualization.py,
    attention heatmap visualization
  - YOLO-Pose regional perception classifier, train_yolo_guided_classifier.py, yolo_guided_evaluator,
    dynamic data sampler
  - Fine-tuning document + Implementation scripts, check_database.py/database_region_analyzer.py/verify_database.py
    to ensure data quality
  metrics_or_impact:
  - Image extraction success rate 100%, typical page with 15 images (8 thumbnails
    + 7 high-definition); ad filtering accuracy 100%, 20% page content removed.
  - Displayed 2,731 images (including 615 manually selected and 945 AI-interested)
    on the web UI and supports batch selection; content hash deduplication maintains
    0 false positives.
  - 'ResNet optimized model validation accuracy: 95.71%; False negatives 3.85% (down
    82.1% from 21.54%), false positives 4.46% (down 71.3%), inference 58.7 images/s'
  - YOLO-Pose region detection accuracy increased from 1% to >90% (+8,900%); classification
    accuracy >96%, false negatives <3%, integrity test passed 8/8.
  - SQLite records 895+ crawling data (245.8 MB) in real-time; all Web API/dashboards,
    SQL, and batch functions pass regression tests
  tech_stack:
  - Python 3.8+ / Requests / BeautifulSoup / Typer-CLI
  - Flask + Jinja2 + Bootstrap + WebSocket + SQLAlchemy
  - SQLite, imagehash, pandas
  - PyTorch 2.x + ResNet18/CBAM + Adaptive Focal Loss + YOLO-Pose (ultralytics)
  - OpenCV / PIL / matplotlib / seaborn
  - TensorBoard/Logging System + pytest/Unit Test
  tools_platforms:
  - "run_image_crawler.py / main.py / config/*.yaml\n translates to:\nrun_image_crawler.py\
    \ / main.py / config/*.yaml"
  - start_web_app.py / web/app.py / routes/api
  - collect_interested_images.py / batch_analyze_pages.py / database_region_analyzer.py
    / check_database.py
  - image_classifier_with_visualization.py / batch_predict_bbox_attention.py / train_bbox_attention.py
  - ml/scripts/train_optimized_model.py / train_yolo_guided_classifier.py / fine_tune_bbox_attention.py
    / yolo_guided_evaluator.py
  - Attention_heatmap generator, YOLO-Pose model, logs & outputs directory
  team_info:
    team_size: 1
    description: Individual independent development, covering crawler, Web, database,
      and CV models.
  notes: See details in D:/0-development/projects/MyFirst (MyFirst.code-workspace
    + CLAUDE/YOLO/Fine-tuning documents).
- project_name: Remedium (BI)
  company_or_context: Portable Water Craft
  timeframe:
    label: 2016.10 - 2023.02
    start: 2016-10
    end: 2023-02
  role_title: Technical Team Leader
  role_perspective: architect
  management_scope:
    team_size: null
    budget_level: null
    stakeholder_tiers:
    - exec
    - director
    - ops
    - regulator
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - risk_governance
  responsibility_focus:
  - architecture
  - stakeholder_management
  - compliance
  impact_metrics:
    business_metrics:
    - User satisfaction reached 95% and passed all compliance audits.
    technical_metrics:
    - Data processing time reduced by 30%.
    - Data quality improved to 99.9%
    operational_metrics:
    - Establish a 24-hour cross-border collaboration rhythm.
  governance_artifacts:
  - training_plan
  - risk_register
  project_overview: A BI solution tailored for the pharmaceutical industry, covering
    data pipeline, report analysis, and web portal.
  data_domain: Pharmaceutical Industry BI
  ai_component_flag: false
  challenges_or_objectives:
  - Design scalable solution architecture
  - Enhance data processing efficiency and reporting timeliness.
  - Enhance business personnel's review experience.
  responsibilities:
  - Design and coordinate cross-border teams.
  - Supervise ADF + Databricks data pipeline development
  - Guidance Report/Implementation of Analysis Layer and Portal Design
  - Promote cross-functional communication
  architecture_or_solution:
  - ADF Pipeline + Databricks Processing Framework
  - Web portal improves review efficiency
  process_or_methodology:
  - Cross-team collaboration and solution review
  deliverables_or_features:
  - Trading data reporting system
  - Remedium Web Portal
  metrics_or_impact:
  - Processing time reduced, data quality improved.
  - Enhanced real-time access to improve problem-solving efficiency.
  - Gained positive feedback from stakeholders.
  tech_stack:
  - Azure Data Factory
  - Databricks
  - BI Tools
  - Web technology
  tools_platforms:
  - Azure Ecosystem
  team_info:
    team_size: null
    description: Cross-border team collaboration
  notes: null
- project_name: SMART
  company_or_context: Personal Watercraft
  timeframe:
    label: 2016.10 - 2023.2
    start: 2016-10
    end: 2023-02
  role_title: Technical Team Leader
  role_perspective: hybrid
  management_scope:
    team_size: null
    budget_level: null
    stakeholder_tiers:
    - exec
    - director
    - ops
    - regulator
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - people_management
  - risk_governance
  responsibility_focus:
  - architecture
  - operations
  - compliance
  impact_metrics:
    business_metrics:
    - Core business uninterrupted for 18 consecutive months.
    technical_metrics:
    - Achieve 99.99% SLA and improve data processing efficiency by 40%.
    operational_metrics:
    - CI/CD reduces deployment time from 2 hours to 15 minutes.
  governance_artifacts:
  - risk_register
  - runbook
  project_overview: Long-term evolution project of the medical insurance pre-approval
    system, focusing on OLTP/OLAP data models, ETL architecture, and SLA guarantees.
  data_domain: Medical insurance pre-approval
  ai_component_flag: false
  challenges_or_objectives:
  - Governance of 10+ years of legacy systems and complex business logic
  - Refactor ETL layer and enhance performance/reliability
  - Meet high SLA and security compliance requirements
  responsibilities:
  - Leadership in OLTP/OLAP data model design and development
  - Reconstruct the ETL architecture and integrate resource scheduling/memory computing
    modules.
  - Promote testing, performance, and risk control
  - Implement CI/CD and DevOps processes
  - Maintain data warehouse and upgrade security
  architecture_or_solution:
  - Redesign the ETL layer and distributed memory engine
  - CI/CD-driven delivery pipeline
  - Security scanning and risk control mechanism
  process_or_methodology:
  - Agile development
  - DevOps + Automated Unit Testing
  - Risk management and performance testing collaboration
  deliverables_or_features:
  - New ETL architecture
  - CI/CD Pipeline
  - Security Upgrade and Risk Control Package
  metrics_or_impact:
  - Achieve 99.99% SLA
  - Enhance data processing efficiency and ensure quality.
  - Reduce vulnerabilities and improve development efficiency.
  tech_stack:
  - Online Transaction Processing / Online Analytical Processing
  - ETL
  - 'CI/CD: Continuous Integration/Continuous Deployment'
  - Agile development
  - Security scanning tool
  tools_platforms:
  - Resource scheduling module
  - Distributed Memory Engine
  team_info:
    team_size: null
    description: null
  notes: null
- project_name: Learning Management System
  company_or_context: HP
  timeframe:
    label: October 2014 - October 2016
    start: 2014-10
    end: 2016-10
  role_title: Leader / Scrum Master
  role_perspective: project_manager
  management_scope:
    team_size: 20
    budget_level: null
    stakeholder_tiers:
    - director
    - ops
    - vendor
  decision_accountability:
  - delivery_owner
  - people_management
  - risk_governance
  responsibility_focus:
  - planning
  - stakeholder_management
  - operations
  impact_metrics:
    business_metrics:
    - Core processes supporting HP's global training and external order management
    technical_metrics:
    - SABA interface data synchronization accuracy reaches 99.8%.
    operational_metrics:
    - Delivery rate increased from 60% to 95%.
    - Team conflict rate down 80%.
  governance_artifacts:
  - roadmap
  - risk_register
  - jira_board
  project_overview: LMS ODS system responsible for HP training and order management,
    connecting SABA training system with HP EDW.
  data_domain: Enterprise Learning and Training Data
  ai_component_flag: false
  challenges_or_objectives:
  - Ensure real-time synchronization with the SABA system.
  - Improve ODS/ETL processing efficiency
  - Coordinate multiple teams and maintain agile delivery
  responsibilities:
  - Design system architecture and API interfaces
  - Develop Informatica ETL jobs
  - Act as a Scrum Master to organize iterations/reviews
  - Resolve team conflicts and make technical decisions.
  architecture_or_solution:
  - SABA API integration + real-time synchronization
  - Informatica Customized Plan Tasks and Data Flows
  - Monitor service quickly locates interface issues
  process_or_methodology:
  - Scrum management
  - Parallel development and task priority control
  deliverables_or_features:
  - Interface monitoring service
  - Incremental extraction mechanism
  - Reorganize team structure (20 people → multiple groups)
  metrics_or_impact:
  - Enhance interface stability and data synchronization efficiency.
  - Ensure continuous delivery for each Sprint
  tech_stack:
  - SABA
  - Informatica
  - ETL (Extract, Transform, Load)
  - Scrum
  - API Development
  tools_platforms:
  - HP Enterprise Data Warehouse
  team_info:
    team_size: 20
    description: Reorganized into a 6-person development team, divided into interface/ETL/ODS/testing.
  notes: null
- project_name: Move to HP Cloud
  company_or_context: HP
  timeframe:
    label: October 2012 - October 2014
    start: 2012-10
    end: 2014-10
  role_title: Senior Developer II
  role_perspective: developer
  management_scope:
    team_size: null
    budget_level: null
    stakeholder_tiers:
    - ops
    - vendor
  decision_accountability:
  - technical_strategy
  - hands_on_build
  responsibility_focus:
  - architecture
  - implementation
  - operations
  impact_metrics:
    business_metrics: []
    technical_metrics:
    - Multi-layered security and load balancing design ensure stable operation after
      migration.
    operational_metrics:
    - Standardized deployment scripts shorten the time for environment setup and migration.
  governance_artifacts:
  - runbook
  project_overview: Migrate HP internal projects to the in-house cloud platform, covering
    infrastructure setup, environment upgrade, and SaaS integration.
  data_domain: Enterprise cloud migration
  ai_component_flag: false
  challenges_or_objectives:
  - Establish a secure and reliable IaaS infrastructure.
  - Unified development/test/production environment deployment
  - Integrate multiple internal SaaS services
  responsibilities:
  - Establish virtual networks, firewalls, load balancing, and virtual machines.
  - Perform installation and upgrade in each environment.
  - Integrate internal SaaS and support testing/regression
  - Resolve obstacles and technical issues during the migration process.
  architecture_or_solution:
  - HP Cloud IaaS Resource Orchestration
  - Multi-layer Load Balancing and Security Gateway Configuration
  process_or_methodology:
  - Environmental consistency and migration test process
  deliverables_or_features:
  - Standardized environment deployment script
  - SaaS Integration Solution
  metrics_or_impact:
  - Ensure the smooth transition and stable operation of the project.
  tech_stack:
  - HP Cloud Infrastructure as a Service
  - Virtual network
  - Load balancing
  - SaaS integration
  tools_platforms:
  - HP Internal Cloud Platform
  team_info:
    team_size: null
    description: null
  notes: null
- project_name: Customer insights
  company_or_context: HP
  timeframe:
    label: 2010.10 - 2014.10
    start: 2010-10
    end: 2014-10
  role_title: Development and design
  role_perspective: developer
  management_scope:
    team_size: null
    budget_level: null
    stakeholder_tiers:
    - customer
    - ops
  decision_accountability:
  - technical_strategy
  - hands_on_build
  - commercial_strategy
  responsibility_focus:
  - architecture
  - implementation
  - commercialization
  impact_metrics:
    business_metrics:
    - Customer-server configuration efficiency significantly improved.
    technical_metrics:
    - Multi-tendency configuration model supports personalized recommendation
    operational_metrics: []
  governance_artifacts: []
  project_overview: Intelligent configuration and recommendation system based on historical
    server configuration data, to help customers quickly generate server solutions.
  data_domain: Enterprise server configuration recommendations
  ai_component_flag: false
  challenges_or_objectives:
  - Design various customer preference models and configuration algorithms.
  - Ensure the accuracy of statistical reports and configuration guidance.
  responsibilities:
  - Develop configuration algorithms and propensity models
  - Establish regular data statistics and reporting.
  - Design UI flow to guide users in configuration
  architecture_or_solution:
  - Trend-driven configuration algorithm + Data statistics engine
  - UI guidance and report feedback loop
  process_or_methodology:
  - Data-driven configuration strategy
  deliverables_or_features:
  - Multi-tendency configuration set and visualization report
  metrics_or_impact:
  - Enhance customer configuration efficiency
  - Support multiple optimization strategies and provide decision analysis
  tech_stack:
  - Configuration algorithm
  - Database
  - UI Development
  - Report system
  tools_platforms:
  - []
  team_info:
    team_size: null
    description: null
  notes: null
