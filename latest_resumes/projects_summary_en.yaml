schema_version: 1.1
generated_at: '2025-11-28T10:40:46.354834'
source_file: Resume_Data_Engineer_CN_20250529.md
projects:
- project_name: MySixth Tarot Card Intelligent Application
  company_or_context: Zoetis / Individual Independent Developer
  timeframe:
    label: October 2025 - Present
    start: 2025-10
    end: null
  role_title: Independent developer
  role_perspective: hybrid
  llm_primary_role: full_stack
  llm_secondary_roles:
  - product_manager
  management_scope:
    team_size: 1
    budget_level: lt_100k
    stakeholder_tiers:
    - customer
    - ops
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - hands_on_build
  - commercial_strategy
  - risk_governance
  responsibility_focus:
  - planning
  - architecture
  - implementation
  - commercialization
  - stakeholder_management
  impact_metrics:
    business_metrics:
    - 'Q4 2025 paid conversion 18%, about 50% above industry benchmark, with 6% tiered daily active payer rate.'
    - 'Locked in RMB 60K ARR from subscriptions and redemption codes within 60 days of launch and established a repeat purchase funnel.'
    technical_metrics:
    - 'Unified LLM factory + cost dashboard lowered AI call cost by 60% while keeping
      TP99 inference under 1.2s.'
    - 'Event bus + cohort dashboards visualize experiment deltas within 24 hours
      to support prompt/paywall tests.'
    operational_metrics:
    - 'Single cloud server deployment keeps monthly operating cost under RMB 500.'
    - 'Android/iOS/Web/Admin suite ships once with shared release automation to accelerate
      cadence.'
    - 'A/B cycle <2 days with feedback loops closed within 48 hours.'
  governance_artifacts:
  - runbook
  - prompt_playbook
  - cost_dashboard
  - ab_test_matrix
  project_overview: 'TarotAI omnichannel kit (Expo React Native app + FastAPI backend
    + Next.js admin + AI content generator) layered with product telemetry, cohort
    dashboards, and prompt governance so anonymous users get a four-step tarot journey
    plus paid AI readings. GitHub: https://github.com/bin448482/tarotAI'
  data_domain: Consumer-level Tarot Fortune Telling / AI Content Operation
  ai_component_flag: true
  challenges_or_objectives:
  - Deliver mobile, backend, admin, and AI generator assets as a solo builder while
    still reserving capacity for PM rituals.
  - Merge user interviews + funnel telemetry into roadmap choices so anonymous identity,
    offline sync, and multi-channel payments line up in one architecture.
  - Build a two-phase AI pipeline, multi-LLM routing, and cost guardrails under a
    unified dashboard.
  - Design subscription/IAP/redemption monetization and codify prompt governance
    plus experimentation guardrails.
  responsibilities:
  - 'Lead four-surface architecture: Expo mobile, FastAPI services, Next.js admin,
    AI batch generator.'
  - 'Define north star metrics (conversion, retention), maintain experiment backlog,
    user interviews, and prioritization cadences.'
  - Design anonymous installation ID + JWT authentication, four-step divination flow,
    and end-to-end API contract.
  - Implement Google Play IAP/redemption/Stripe (reserved) charging routes, revenue
    share reports, and risk controls.
  - Set up Docker Compose + Nginx deployment, monitoring, and offline backups while
    chairing prompt governance and risk reviews.
  - Publish multilingual README/CLAUDE guides and telemetry SOPs to institutionalize
    build + operate processes.
  architecture_or_solution:
  - Expo RN (SDK54) client + FastAPI monolith + Next.js 15 admin + Python AI generator
    layered architecture.
  - 'Dual-stage AI API: `/readings/analyze` for recommendations, `/readings/generate`
    for paid narratives, backed by prompt audit trail.'
  - 'Anonymous identity: `installation_id` + optional email binding + JWT access,
    plus session enrichment for cohort tagging.'
  - 'Offline-first: Expo SQLite `tarot_config.db` preload + sync strategy, backend
    SQLite persistence.'
  - 'Payments: Google Play Billing, bulk redemption codes, reserved Stripe Checkout
    with revenue share dashboards.'
  - Docker Compose (backend/admin/nginx) + Nginx route split.
  - 'AI content ops: `tarot-ai-generator` Typer CLI linking SQLite + GLM-4/OpenAI/Ollama
    for batch copy with spend monitors.'
  - 'Product telemetry: PostHog/Amplitude events feed cohort + retention dashboards
    and experiment decisions.'
  process_or_methodology:
  - Vibe coding / modular components + Expo Router four-step flow.
  - OKR → north star metrics → hypothesis backlog → A/B validation cadence layered
    onto TypeScript/Python coding standards.
  - Offline sync + SQLite versioning + Docker volume backups.
  - EAS Build + CI/CD + multi-environment `.env` governance.
  - Doc-driven development (README/CLAUDE) + prompt governance reviews.
  - Weekly user interviews/feedback tagging feeding the experiment matrix.
  deliverables_or_features:
  - 'Expo mobile app `my-tarot-app`: spreads, AI paid readings, history, icon pipeline.'
  - 'FastAPI `tarot-backend`: LLM gateway, payments, admin APIs, static assets, health
    checks.'
  - 'Next.js `tarot-admin-web`: dashboards, user/points ops, bulk code generation,
    order tracking, client download portal.'
  - 'Tarot AI Generator: multimode batch generation, multilingual routing, JSON import/export.'
  - 'Product telemetry + cohort dashboards: funnel/retention/pay analysis with cost/revenue
    reconciliation.'
  - 'Prompt governance pack: templates, risk checklist, A/B log, cost ceilings.'
  - Nginx + Docker Compose deployment topology, offline SQLite backup manual.
  - "Monetization strategy: IAP first → redemption/Stripe fallback with anonymous identity compatibility."
  metrics_or_impact:
  - Production suite running on Android/iOS/Web/Admin.
  - Two-stage AI narrative lifts conversion and achieves 34% month-2 retention.
  - Anonymous identity + offline cache enables journeys without registration or
    strong connectivity.
  - Dockerized delivery shortens environment setup, SQLite backup lowers ops risk.
  - Experiment matrix surfaced high-value spreads, improving monthly retention by
    12 pts and ARR by RMB 18K.
  tech_stack:
  - Expo React Native (SDK 54 / React Native 0.81)
  - TypeScript 5.x
  - Next.js 15 App Router + Ant Design 6 + Tailwind CSS
  - FastAPI 0.104 + SQLAlchemy + Uvicorn
  - SQLite (Expo/Backend)
  - Python 3.10 + Typer CLI
  - GLM-4 (ZhiPuAI)
  - OpenAI API
  - Ollama/Claude (removable)
  - LangChain / Self-developed LLM Router
  - Docker & Docker Compose
  - Nginx Proxy
  - Google Play Billing / Stripe Checkout (Reserved)
  tools_platforms:
  - Expo Router 6
  - Zustand + SWR
  - EAS Build
  - Ant Design Charts
  - ESLint / Pylint / Prettier
  - CI/CD Pipeline + generate-icons.js script
  - Docker volume backup solution
  team_info:
    team_size: 1
    description: Personal independent development + AI collaboration + multi-repo, documentation-driven
  notes: 'See tarotAI repo (Expo app, FastAPI backend, Next.js admin, AI generator,
    Docker/Nginx) plus user interview and prompt governance notes. GitHub: https://github.com/bin448482/tarotAI'
- project_name: MyFirst Vertical-site Image Crawler and AI Vision Filtering System
  company_or_context: Personal independent project / self-built image data pipeline
  timeframe:
    label: August 2025
    start: 2025-08
    end: 2025-08
  role_title: Independent Developer & Computer Vision Engineer
  role_perspective: developer
  llm_primary_role: ai_engineer
  llm_secondary_roles:
  - data_development
  - ai_development
  - full_stack
  management_scope:
    team_size: 1
    budget_level: lt_100k
    stakeholder_tiers:
    - ops
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - hands_on_build
  responsibility_focus:
  - architecture
  - implementation
  - operations
  impact_metrics:
    business_metrics:
    - 'Replaced 80%+ of manual image-by-image review with AI-assisted filtering, cutting daily screening time from ~2 hours to under 30 minutes while supplying high-quality training data for downstream CV projects.'
    - 'Built a reusable crawl → filter → label → train loop that significantly reduces preparation overhead for fine-tuning and transfer learning experiments.'
    technical_metrics:
    - 'ResNet50 + CBAM classifier reaches 95.71% validation accuracy with 3.85% false negative rate and 4.46% false positive rate, clearly outperforming the baseline model.'
    - 'Bounding-box attention + YOLO-Pose region awareness improved inference throughput by ~30% while reducing memory usage by ~25%.'
    operational_metrics:
    - 'SHA-256 content hashing driven deduplication sharply reduced duplicate image writes with 100% extraction success on typical pages.'
    - 'Flask web console stably hosts tens of thousands of crawl records and several GB of image data for real-time review and labeling.'
  governance_artifacts:
  - runbook
  - training_plan
  project_overview: 'An end-to-end image pipeline for a specific vertical site: a customized crawler that handles non-standard HTML attributes, consolidated SQLite storage with deduplication, a Flask web console, and ResNet/YOLO-Pose-based vision models that automate “interesting image” selection and support anti-overfitting fine-tuning experiments.'
  data_domain: Vertical-site image crawling / content filtering / computer vision
  ai_component_flag: true
  challenges_or_objectives:
  - 'Robustly extract images from non-standard HTML (ess-data / data-link attributes) while keeping the crawler configurable.'
  - 'Wire crawler, SQLite store, web console, and ML training datasets into a single end-to-end workflow to reduce data wrangling cost.'
  - 'Design a high-recall classifier that keeps false negatives low in a sensitive screening scenario.'
  - 'Mitigate severe overfitting under limited data via color augmentation, sample balancing, and noisy sample injection.'
  responsibilities:
  - 'Refactored the vertical-site specific crawler and YAML configs to handle ess-data/data-link attributes and maintain an ad URL blacklist.'
  - 'Designed SQLite schemas and deduplication logic to centralize crawl_results storage and feed the web console.'
  - 'Implemented the ResNet50 + CBAM classification pipeline, bounding-box attention module, and YOLO-Pose region-awareness flow.'
  - 'Authored fine-tuning scripts (train/evaluate/visualize) integrating color augmentation, sample rebalancing, noise injection, and multi-level fallback logic.'
  - 'Built the Flask web console and APIs (dashboard, SQL console, image gallery), enabling manual + AI-assisted labeling and filtering.'
  architecture_or_solution:
  - 'Modular architecture: crawler (requests/custom parsing) → SQLite storage → ML training datasets → Flask web console.'
  - 'Custom YAML crawler configs using CSS selectors on img[ess-data]/img[data-link] plus ad URL blacklist support.'
  - 'Content-hash (SHA-256) and perceptual-hash based deduplication and image naming with date-based directory layout.'
  - 'ResNet50 + CBAM classifier with adaptive focal loss and dynamic thresholding tuned to reduce false negatives.'
  - 'YOLOv8 Pose–driven body-region detection plus bounding-box attention to keep accuracy while improving efficiency and robustness.'
  - 'Fine-tuning architecture with SmartColorAugmentation, DynamicSampleBalancer, NoiseDataInjector, and OverfittingMonitor components.'
  process_or_methodology:
  - 'Experiment-log and Markdown-driven iteration (CLAUDE.md + fine_tuning_* docs) to progressively stabilize the CV solution design.'
  - 'Automated reports via evaluate_fine_tuning.py (training/validation curves, confusion matrices, metrics) to support model selection and regression checks.'
  - 'Progressive unfreezing and layer-wise learning rates combined with dynamic sampling to balance convergence speed and overfitting risk.'
  - 'Scripted regression checks (run_tests.py) around crawler configs and web console endpoints to keep critical paths stable.'
  deliverables_or_features:
  - 'Vertical-site specific image crawler + YAML configs (including ad filtering, broken URL handling, and batch page analysis tools).'
  - 'Unified SQLite storage layer and deduplication utilities powering the crawl_results table and web queries.'
  - 'Flask web console: dashboard, SQL query console, image gallery, config editor, and live status views.'
  - 'ResNet50 + CBAM classifier and inference scripts supporting batch prediction and prediction-delta analysis.'
  - 'Bounding-box attention training/inference pipelines and visualization tools for attention heatmaps and region-level statistics.'
  - 'Fine-tuning usage guide and configuration examples packaging color augmentation, sample balancing, and noise injection strategies.'
  metrics_or_impact:
  - 'Achieved 100% extraction success on typical target pages with ~15 images per page (thumbnails + HD).'
  - 'Ad-blocking list filters specified URLs with 100% precision (~20% block ratio) without harming normal downloads.'
  - 'Optimized ResNet50 model reaches 95.71% validation accuracy while cutting false negative rate from 21.54% to 3.85%.'
  - 'Bounding-box attention and lightweight attention modules improve inference speed by ~30% with ~25% lower memory usage.'
  - 'Web console reliably manages tens of thousands of crawl records and several GB of images, enabling joint manual + AI curation of interesting samples.'
  tech_stack:
  - Python 3.10
  - Requests / BeautifulSoup / custom HTML parsing
  - Flask / Jinja2 / WebSocket
  - SQLite / SQLAlchemy
  - PyTorch / torchvision / ResNet50
  - YOLOv8 Pose (ultralytics)
  - imagehash / OpenCV / Pillow
  tools_platforms:
  - pytest / run_tests.py
  - matplotlib / seaborn
  - VS Code / Jupyter Notebook
  - Local Git repo
  team_info:
    team_size: 1
    description: Sole owner of crawler, web console, database, and vision training pipeline.
  notes: 'See MyFirst repo (vertical-site crawler, SQLite storage, Flask web console, ResNet/YOLO-Pose models, fine-tuning toolkit) for full implementation and experiment logs.'
- project_name: NGSE (Next Generation Sales Engine)
  company_or_context: Zoetis
  timeframe:
    label: 2023.6 - 2025.9
    start: 2023-06
    end: 2025-09
  role_title: Project Manager & Data Architect & Generative AI Product Lead
  role_perspective: project_manager
  llm_primary_role: data_development
  llm_secondary_roles:
  - ai_development
  - product_manager
  - ai_product_designer
  - ai_engineer
  management_scope:
    team_size: 5
    budget_level: gt_1m
    stakeholder_tiers:
    - exec
    - director
    - ops
    - vendor
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - people_management
  - risk_governance
  - commercial_strategy
  responsibility_focus:
  - planning
  - architecture
  - stakeholder_management
  - compliance
  - commercialization
  impact_metrics:
    business_metrics:
    - Sales playbooks + AI opportunity recommendations reached 82% adoption and delivered RMB 28M ARR uplift.
    - Regional pipeline visibility improved 35% with exec dashboard NPS +40.
    technical_metrics:
    - Key report runtime improved 2-3x while LLM summaries stay under 4 seconds of latency.
    - Data accuracy hit 99.5% to power AI training and multi-market reuse.
    operational_metrics:
    - Enterprise lakehouse + experiment handbook scaled to additional markets.
    - LLM review + prompt QA flow cut risk defect closure time by 45%.
  governance_artifacts:
  - exec_dashboard
  - risk_register
  - prompt_playbook
  - ai_policy
  project_overview: "Delivered a compliant AI engine + lakehouse foundation for Zoetis' global NGSE program in China, solving cross-border data restrictions, model definition gaps, and recommendation design so local sellers can reuse global playbooks and quantify monetization."
  data_domain: Global Sales Engine / Data Platform
  ai_component_flag: true
  challenges_or_objectives:
  - "Construct an auditable local NGSE data + AI stack under China's cross-border data regulations."
  - "Map Zoetis global canonical models to domestic distributor/sales schemas with aligned fields, granularity, and KPIs."
  - "Validate whether China sales/inventory data is fit for AI training, prompt engineering, and recommendation quality."
  - "Design explainable, compliant recommendation algorithms and prompt governance that keep sales playbooks reusable."
  responsibilities:
  - "Built the Inbox→Raw→Transform→Governed four-layer lakehouse handling CSV/JSON/Parquet/Avro, replay, and audit to host China mirror datasets."
  - "Decomposed ingestion/cleansing/join/aggregation pipelines in Databricks Workflows with global_temp/temp/intermediate tables and Delta time-travel for debugging."
  - "Manually registered Delta catalogs + SQL Server mappings to unify metadata/permissions and enforce T-1 full + incremental variance checks."
  - "Authored/tuned Spark SQL & PySpark reusable ETL (encryption, masking, path parsing, dynamic params) plus parameterized notebook templates with Widgets for rapid market rollout."
  - "Integrated ADF, Azure Functions, Databricks REST API, and Aliyun OSS→ADLS Gen2 transfer for cross-platform orchestration, auto email/Teams validation notices, and business-triggered jobs."
  - "Owned Sales Copilot recommendation design + prompt QA rituals, running exec/risk/commercial retros."
  - "Led domestic data acquisition, field mapping, batch validation, and creation of training/test/validation datasets for recommendation model regression."
  architecture_or_solution:
  - "Four-layer architecture: Inbox (multi-format ingress) → Raw (standardization/naming) → Transform (cleansing/joins/aggregation) → Governed (lineage/quality/security-ready publication)."
  - "Databricks Workflows modules per ingestion/cleansing/join/aggregation stage with global_temp/temp/intermediate table strategy."
  - "Delta Lake versioning + ACID enabling batch rewind, full/incremental switching, and automatic T-1 variance checks on sales/product flow wide tables."
  - "Metadata + method management via manual Delta catalog registration, SQL Server mappings, method registry, and Hive SQL auto-loader."
  - "Parameterized notebook templates with Widgets + shared ETL components (encryption/masking/path parsing/dynamic params) codifying company lakehouse standards."
  - "Azure Function ↔ Databricks loop sending validation emails/Teams alerts and letting business uploads trigger jobs."
  - "ADF owning Linked Services/Integration Runtime, Aliyun OSS→ADLS Gen2 transfers, and scheduled Databricks notebooks."
  - "Lakehouse + Auto Loader + dual consumption (AI training + Power BI/Fabric real-time queries) with unified wide tables."
  - "Sales Copilot recommendation service delivering LLM summaries, next actions, risk prompts, and chat embedded in Power BI/CRM."
  process_or_methodology:
  - "Lakehouse + PromptOps + data export compliance tri-track governance with authorization/masking policies."
  - "Modular/atomic notebook, ETL, and method registry standards (DNA architecture) enabling pluggable dimensions/facts."
  - "Automated data quality (schema drift, field completeness, null/duplicate) with Azure Function alert loops."
  - "Biweekly OKR/roadmap, recommendation algorithm retros, and risk reviews for exec/ops alignment."
  deliverables_or_features:
  - "Databricks pipeline blueprint covering ingestion/cleansing/join/aggregation modules and global_temp/temp/intermediate table governance."
  - "Metadata catalog (Delta→SQL Server mapping) + method registry with reusable encryption/masking/path parsing/dynamic parameter modules."
  - "Schema drift, field completeness, null/duplicate monitoring plus custom validation rule library."
  - "Azure Function-driven validation email/Teams alerts + Databricks job trigger loop."
  - "Aliyun OSS→ADLS Gen2 transfer pipeline alongside Databricks REST API deployment automation."
  - "Sales Copilot recommendation engine + dual-channel consumption (AI training + Power BI real-time)."
  - "Training/test/validation dataset packs with validation scripts for model regression and regulatory sampling."
  metrics_or_impact:
  - "Inventory/sales reports ran 2-3x faster, meeting AI training + BI SLA targets."
  - "T-1 variance checks across sales/product flow tables kept accuracy at 99.5%."
  - "AI recommendation hit rate 70%, saving sellers six hours weekly on lead prep."
  - "Lakehouse + prompt governance replicated in four markets with zero high-severity risk escalations and first-pass compliance approvals."
  tech_stack:
  - Databricks
  - Delta Lake
  - Azure Data Factory
  - Azure Data Lake Storage Gen2
  - Aliyun OSS
  - Spark SQL
  - PySpark
  - Azure Functions
  - Power BI + Fabric
  - Azure OpenAI / GLM API
  - SQL Server
  tools_platforms:
  - Databricks Workflows
  - ADF
  - Azure Data Lake Storage Gen2
  - Azure Function Apps
  - Aliyun OSS
  - Prompt QA toolkit
  - Databricks REST API
  team_info:
    team_size: 5
    description: Cross-market squad mixing PO, data engineers, analysts.
  notes: Owned China market roadmap + PromptOps governance adopted as a global reference.
- project_name: Remedium (BI)
  company_or_context: PwC
  timeframe:
    label: 2016.10 - 2023.02
    start: 2016-10
    end: 2023-02
  role_title: Senior Technical Consultant (Solutions Architect)
  role_perspective: architect
  llm_primary_role: data_development
  llm_secondary_roles:
  - full_stack
  management_scope:
    team_size: null
    budget_level: null
    stakeholder_tiers:
    - exec
    - director
    - ops
    - regulator
  decision_accountability:
  - delivery_owner
  - technical_strategy
  - risk_governance
  responsibility_focus:
  - architecture
  - stakeholder_management
  - compliance
  impact_metrics:
    business_metrics:
    - Passed every regulator audit on the first attempt and shortened approval wait time via transparent reporting.
    technical_metrics:
    - ADF + Databricks parallel ingestion kept SAP/TPPT batch windows within SLA.
    - Layered data model + quality gates sustained 99%+ master data accuracy.
    operational_metrics:
    - Established a 24-hour EU/CN follow-the-sun rhythm with structured post-incident reviews.
  governance_artifacts:
  - training_plan
  - risk_register
  project_overview: Remedium is a pharma-focused BI platform that unifies multi-country MDM data, builds auditable ADF + Databricks pipelines and data models, and exposes transparent workflows/reporting through a web portal.
  data_domain: Pharmaceutical Industry BI
  ai_component_flag: false
  challenges_or_objectives:
  - Consolidate SAP, TPPT, and research feeds into a scalable cross-region MDM backbone.
  - Remove big-file ingestion bottlenecks and harden the ADF + Databricks ETL layer.
  - Provide auditors and business teams with self-serve transparency via a portal.
  responsibilities:
  - Co-lead requirement grooming, solution reviews, and delivery roadmap with onshore teams.
  - Own ADF + Databricks ETL standardization across SAP/TPPT/research sources plus monitoring.
  - Design multithreaded/sharded ingestion patterns and internal tooling to boost performance and developer efficiency.
  - Deliver reusable data models and business process templates per client architecture.
  - Plan and ship the Remedium web portal so stakeholders can track workflows and generate transparent reports.
  architecture_or_solution:
  - Layered data model: core MDM backbone with client-specific extension domains.
  - ADF pipelines + Databricks jobs powering parallel ingestion and compute.
  - Configurable ETL toolbox with source adapters, data quality checks, and performance telemetry.
  - Remedium web portal + reporting APIs aligned to audit workflows.
  process_or_methodology:
  - Requirement → architecture → model review workshops with weekly onshore syncs.
  - Data standardization and process template governance for cross-region consistency.
  - Performance regression drills and ingestion dry-runs coupled with post-mortems.
  deliverables_or_features:
  - Cross-region MDM ETL packages and source adapters (SAP, TPPT, research data).
  - ADF/Databricks batch jobs plus internal productivity tooling.
  - Client-specific data model blueprints and standardized business process packs.
  - Remedium web portal with transparent workflow tracking and audit report generation.
  metrics_or_impact:
  - Met multi-region MDM batch SLAs on ADF/Databricks even with large files.
  - Standardized models accelerated new-client onboarding and alignment.
  - Portal transparency cut manual escalations and sped auditor-business collaboration.
  tech_stack:
  - Azure Data Factory
  - Databricks
  - Microsoft SQL Server
  - SAP / TPPT data sources
  - MDM platform
  - .NET / web portal stack
  tools_platforms:
  - ADF Pipeline & Monitor
  - Databricks Workflows
  - SQL Server Agent
  - IIS / internal web portal
  - Performance monitoring & logging toolkit
  team_info:
    team_size: null
    description: Built jointly with European onshore and local delivery teams.
  notes: Owned Remedium architecture/ETL/portal delivery for pharma compliance transparency.
