schema_version: 1.1
# schema_fields:
#   role_perspective: developer|architect|project_manager|product_owner|hybrid
#   management_scope:
#     team_size: integer or null
#     budget_level: lt_100k|bt_100k_1m|gt_1m|null
#     stakeholder_tiers: [exec, director, ops, vendor, ...]
#   decision_accountability: [delivery_owner, technical_strategy, people_management, hands_on_build, commercial_strategy, risk_governance]
#   responsibility_focus: [planning, architecture, implementation, operations, commercialization, stakeholder_management, compliance]
#   impact_metrics:
#     business_metrics: []
#     technical_metrics: []
#     operational_metrics: []
#   governance_artifacts: []
generated_at: "2025-11-27"
source_file: "Resume_Data_Engineer_CN_20250529.md"
projects:
  - project_name: "塔罗牌智能应用"
    company_or_context: "Zoetis / 个人独立开发者"
    timeframe:
      label: "2025.10 - 至今"
      start: "2025-10"
      end: null
    role_title: "独立开发者"
    role_perspective: hybrid
    llm_primary_role: full_stack
    llm_secondary_roles:
    - product_manager
    management_scope:
      team_size: 1
      budget_level: lt_100k
      stakeholder_tiers:
      - customer
      - ops
    decision_accountability:
    - delivery_owner
    - technical_strategy
    - hands_on_build
    - commercial_strategy
    - risk_governance
    responsibility_focus:
    - planning
    - architecture
    - implementation
    - commercialization
    - stakeholder_management
    impact_metrics:
      business_metrics:
      - "Q4 2025 付费转化率 18%，较行业均值 +50%，日活分层付费率 6%"
      - "上线60天即锁定 6 万人民币 ARR 的订阅/兑换码流水，并形成复购漏斗"
      technical_metrics:
      - "统一 LLM 工厂 + 成本看板让 AI 调用成本降低 60%，高峰期 TP99 < 1.2s"
      - "事件总线 + Cohort 仪表盘将实验指标在 24 小时内可视化，支撑提示/付费实验"
      operational_metrics:
      - "单台云服务器部署将月运营成本控制在 500 元以内"
      - "一次交付 Android/iOS/Web/Admin 四端套件，加速版本迭代"
      - "A/B 实验迭代周期 < 2 天，用户反馈闭环 48 小时内完成"
    governance_artifacts:
    - runbook
    - prompt_playbook
    - cost_dashboard
    - ab_test_matrix
    project_overview: "TarotAI 全渠道套件：Expo React Native 客户端 + FastAPI 后端 + Next.js 管理台 + AI 内容生成工具，叠加产品遥测、Cohort 仪表盘与提示治理，面向匿名用户提供四步塔罗体验与付费 AI 解读。GitHub: https://github.com/bin448482/tarotAI"
    data_domain: "消费级塔罗占卜 / AI内容运营"
    ai_component_flag: true
    challenges_or_objectives:
      - "在单人团队下同时交付移动端、后端、管理后台与 AI 生成工具"
      - "将用户访谈与漏斗数据沉淀到产品决策，确保匿名身份、离线同步和多渠道支付在同一架构协同"
      - "构建双阶段 AI 流水线、多 LLM 路由与成本看板，控制推理成本"
      - "设计订阅/IAP/兑换码多渠道商业化，并形成提示治理与A/B实验纪律"
    responsibilities:
      - "主导四端架构：Expo 移动端、FastAPI 服务、Next.js Admin、AI 批量生成工具"
      - "梳理北极星指标（付费率/留存），沉淀实验待办、用户访谈与需求优先级"
      - "设计匿名安装 ID + JWT 认证、四步占卜流程与端到端 API 契约"
      - "实现 Google Play IAP/兑换码/Stripe（预留）的充值路由、分账报表与风控策略"
      - "搭建 Docker Compose + Nginx 部署、监控及离线数据备份策略，维护提示治理与风险评审节奏"
      - "撰写多语言 README/CLAUDE 指南与产品遥测 SOP，固化开发 + 运营流程"
    architecture_or_solution:
      - "Expo RN (SDK54) 客户端 + FastAPI 单体 + Next.js 15 Admin + Python AI Generator 的分层架构"
      - "双阶段 AI API：`/readings/analyze` 推荐维度、`/readings/generate` 输出付费解读，并挂载提示治理清单"
      - "匿名身份体系：`installation_id` + 可选邮箱绑定 + JWT 访问控制 + 会话留资策略"
      - "离线优先：Expo SQLite `tarot_config.db` 预置 + 同步策略，后台 SQLite 独立持久化"
      - "支付与渠道：Google Play Billing、兑换码批量生成、Stripe Checkout 预留接口 + 分账报表"
      - "Docker Compose (backend/admin/nginx) + Nginx 路由 `/api/*` → backend，其余 → admin"
      - "AI 内容运营：`tarot-ai-generator` Typer CLI 连接 SQLite 与 GLM-4/OpenAI/Ollama，实现批量维度文案 + 成本监控"
      - "产品遥测：PostHog/Amplitude 事件 → Cohort + 留存看板 → 实验决策"
    process_or_methodology:
      - "Vibe Coding / 模块化组件体系 + Expo Router 四步流程"
      - "OKR → 北极星指标 → 建议 backlog → A/B 验证的产品节奏，结合 TypeScript/Python 统一编码规范"
      - "离线同步 + SQLite 版本控制 + Docker volume 备份策略"
      - "EAS Build + CI/CD + 多环境 `.env` 管理"
      - "文档驱动开发（README/CLAUDE）与单仓多应用管理 + 提示治理审查会"
      - "每周用户访谈与反馈打标，闭环至实验矩阵"
    deliverables_or_features:
      - "Expo 移动端 `my-tarot-app`：三牌阵/凯尔特十字、AI 付费解读、历史记录、图标脚本"
      - "FastAPI `tarot-backend`：LLM 网关、支付、管理员接口、静态资产、健康检查"
      - "Next.js `tarot-admin-web`：仪表盘、用户/积分运营、兑换码批量生成、订单来源追踪、客户端下载门户"
      - "Tarot AI Generator：多模式批量生成、多语言路由、JSON 导出与导入脚本"
      - "产品遥测 + Cohort 仪表盘：漏斗、留存、付费分析 + 成本/收益对账报表"
      - "提示治理包：提示模板、风险清单、A/B 记录、成本上限告警"
      - "Nginx + Docker Compose 统一部署拓扑、离线 SQLite 卷备份手册"
      - "充值策略：IAP 优先 → 兑换码/Stripe fallback，兼容匿名身份"
    metrics_or_impact:
      - "交付可在 Android/iOS/Web/Admin 同步运行的生产级套件"
      - "双阶段 AI 解读将静态牌义拓展为个性化叙事，支撑付费转化和 34% 次月留存"
      - "匿名身份 + 离线缓存让无注册/弱网场景也能完成四步占卜"
      - "Docker 化部署缩短环境搭建时间，SQLite 卷备份流程降低运维风险"
      - "实验矩阵识别出高价值塔罗阵法，月活留存提升 12pt，ARR 改善 1.8 万元"
    tech_stack:
      - "Expo React Native (SDK 54 / React Native 0.81)"
      - "TypeScript 5.x"
      - "Next.js 15 App Router + Ant Design 6 + Tailwind"
      - "FastAPI 0.104 + SQLAlchemy + Uvicorn"
      - "SQLite (Expo/Backend)"
      - "Python 3.10 + Typer CLI"
      - "GLM-4 (智谱AI)"
      - "OpenAI API"
      - "Ollama/Claude（可插拔）"
      - "LangChain / 自研 LLM Router"
      - "Docker & Docker Compose"
      - "Nginx Proxy"
      - "Google Play Billing / Stripe Checkout (预留)"
    tools_platforms:
      - "Expo Router 6"
      - "Zustand + SWR"
      - "EAS Build"
      - "Ant Design Charts"
      - "ESLint / Pylint / Prettier"
      - "CI/CD Pipeline + scripts/generate-icons.js"
      - "Docker volume 备份方案"
    team_info:
      team_size: 1
      description: "个人独立开发 + AI 协作 + 多仓文档驱动"
    notes: "详细设计参见 tarotAI 仓库（Expo 客户端、FastAPI 后端、Next.js 管理台、AI 生成器、Docker/Nginx 部署），另附用户访谈与提示治理纪要。GitHub: https://github.com/bin448482/tarotAI"

  - project_name: "垂直站点图像爬虫与 AI 视觉筛选系统"
    company_or_context: "个人独立开发者 / 自研图像数据管道"
    timeframe:
      label: "2025.8"
      start: "2025-08"
      end: "2025-08"
    role_title: "独立开发者 & 计算机视觉工程师"
    role_perspective: developer
    llm_primary_role: ai_engineer
    llm_secondary_roles:
    - data_development
    - ai_development
    - full_stack
    management_scope:
      team_size: 1
      budget_level: lt_100k
      stakeholder_tiers:
      - ops
    decision_accountability:
    - delivery_owner
    - technical_strategy
    - hands_on_build
    responsibility_focus:
    - architecture
    - implementation
    - operations
    impact_metrics:
      business_metrics:
      - "利用 AI 自动筛选替代 >80% 手工逐图检查，将每日图像筛选时间从约 2 小时压缩到 <30 分钟，为后续视觉项目持续提供高质量训练数据。"
      - "构建可复用的图像采集→筛选→标注→训练闭环，为 Fine-tuning/迁移学习实验节省大量准备时间。"
      technical_metrics:
      - "ResNet50+CBAM 分类模型在验证集上达到 95.71% 准确率，假阴性率 3.85%，假阳性率 4.46%，显著优于基线模型。"
      - "边界框注意力 + YOLO-Pose 区域感知设计使推理速度提升约 30%，内存占用降低约 25%。"
      operational_metrics:
      - "基于 SHA-256 内容哈希的重复检测将重复图片写入率显著降低，典型页面图像提取成功率达到 100%。"
      - "Flask Web 管理台稳定承载数万条爬取记录与数 GB 级别图像数据，支持实时筛选与标注。"
    governance_artifacts:
    - runbook
    - training_plan
    project_overview: "围绕特定垂直站点构建端到端图像数据管道：定制爬虫适配非标准 HTML 属性，接入 SQLite 存储与重复检测，叠加 Flask Web 管理台和 ResNet/YOLO-Pose 视觉模型，实现对感兴趣图片的自动筛选与过拟合敏感场景下的 Fine-tuning 实验平台。"
    data_domain: "垂直网站图像爬取 / 内容筛选 / 计算机视觉"
    ai_component_flag: true
    challenges_or_objectives:
      - "在非标准 HTML 结构（ess-data / data-link 属性）下稳定提取目标站点图像并构建可配置爬虫。"
      - "打通爬虫、SQLite 存储、Web 管理界面与 ML 训练数据集的端到端闭环，降低数据整理成本。"
      - "设计高召回率的图像分类模型，在敏感场景中尽量降低假阴性率。"
      - "通过颜色增强、样本平衡与噪声注入缓解小样本场景下的严重过拟合。"
    responsibilities:
      - "重构垂直站点专用爬虫与 YAML 配置，支持 ess-data/data-link 图像属性解析与广告 URL 黑名单过滤。"
      - "设计 SQLite 表结构与重复检测逻辑，统一抓取结果存储并与 Web 界面联动。"
      - "实现 ResNet50+CBAM 图像分类管线、边界框注意力模块与 YOLO-Pose 区域感知流程。"
      - "编写 Fine-tuning 相关脚本（训练/评估/可视化），集成颜色增强、样本平衡、噪声注入与多级 Fallback 逻辑。"
      - "搭建 Flask Web 管理台与 API（仪表盘、SQL 控制台、图像画廊），支持感兴趣图片标注与 AI 预测过滤。"
    architecture_or_solution:
      - "模块化架构：Crawler（requests/自定义解析）→ SQLite 存储 → ML 训练数据集 → Flask Web 管理台。"
      - "自定义 YAML 爬虫配置：使用 CSS 选择器提取 img[ess-data]/img[data-link]，并集成广告 URL 黑名单。"
      - "基于 SHA-256 内容哈希与感知哈希的重复检测与图像文件命名策略，按日期目录组织下载数据。"
      - "ResNet50+CBAM 分类器 + 自适应 Focal Loss + 动态阈值优化，聚焦降低假阴性。"
      - "YOLO-Pose 驱动的身体区域检测 + 边界框注意力机制，在保持精度的前提下提升推理效率与鲁棒性。"
      - "Fine-tuning 架构：SmartColorAugmentation、DynamicSampleBalancer、NoiseDataInjector、OverfittingMonitor 等可配置组件。"
    process_or_methodology:
      - "以实验日志和 Markdown 设计文档驱动的迭代（CLAUDE.md + fine_tuning_* 文档），逐步固化视觉方案。"
      - "通过 evaluate_fine_tuning.py 自动生成训练/验证曲线、混淆矩阵与指标报表，支撑模型选择与回归分析。"
      - "引入分阶段解冻与分层学习率策略，配合动态采样，控制过拟合与收敛稳定性。"
      - "对 Web 界面与爬虫配置实行脚本化验证与回归测试（run_tests.py），确保关键路径稳定。"
    deliverables_or_features:
      - "垂直站点专用图像爬虫与 YAML 配置（含广告过滤、异常 URL 处理与批量页面分析工具）。"
      - "统一的 SQLite 存储层与重复检测模块，支撑抓取结果表及 Web 查询。"
      - "Flask Web 管理台：仪表盘、SQL 查询界面、图像画廊、配置管理与实时状态视图。"
      - "ResNet50+CBAM 图像分类模型与推理脚本，支持批量预测与预测差异分析。"
      - "边界框注意力训练/推理管线与可视化工具，输出注意力热力图与区域级别统计。"
      - "Fine-tuning 使用指南与配置示例，封装颜色增强/样本平衡/噪声注入策略。"
    metrics_or_impact:
      - "在典型目标页面上实现 100% 图像提取成功率，每页平均提取约 15 张图像（缩略图+高清）。"
      - "广告过滤名单对指定 URL 的过滤准确率 100%，过滤比约 20%，无误杀正常图像。"
      - "优化后 ResNet50 模型在验证集上达到 95.71% 准确率，假阴性率从 21.54% 降至 3.85%。"
      - "边界框注意力与轻量化注意力模块使推理速度提升约 30%，内存占用降低约 25%。"
      - "Web 管理台稳定托管数万条抓取记录和数 GB 级别图像数据，支持手动与 AI 共同标注感兴趣图像。"
    tech_stack:
      - "Python 3.10"
      - "Requests / BeautifulSoup / 自定义 HTML 解析"
      - "Flask / Jinja2 / WebSocket"
      - "SQLite / SQLAlchemy"
      - "PyTorch / torchvision / ResNet50"
      - "YOLOv8 Pose (ultralytics)"
      - "imagehash / OpenCV / Pillow"
    tools_platforms:
      - "pytest / run_tests.py"
      - "matplotlib / seaborn 报表与可视化"
      - "VS Code / Jupyter Notebook"
      - "Git 本地仓库"
    team_info:
      team_size: 1
      description: "个人独立完成爬虫、Web、数据库与计算机视觉训练全链路。"
    notes: "完整实现与实验记录见本地示例仓库（垂直站点专用爬虫、SQLite 存储、Flask Web 管理台、ResNet/YOLO-Pose 模型与 Fine-tuning 工具链），适合作为 AI 视觉筛选与数据管道项目示例。"

  - project_name: "NGSE（Next Generation Sales Engine）"
    company_or_context: "Zoetis"
    timeframe:
      label: "2023.6 - 2025.9"
      start: "2023-06"
      end: "2025-09"
    role_title: "项目经理 & 数据架构师 & Generative AI 产品负责人"
    role_perspective: project_manager
    llm_primary_role: data_development
    llm_secondary_roles:
    - ai_development
    - product_manager
    - ai_product_designer
    - ai_engineer
    management_scope:
      team_size: 5
      budget_level: gt_1m
      stakeholder_tiers:
      - exec
      - director
      - ops
      - vendor
    decision_accountability:
    - delivery_owner
    - technical_strategy
    - people_management
    - risk_governance
    - commercial_strategy
    responsibility_focus:
    - planning
    - architecture
    - stakeholder_management
    - compliance
    - commercialization
    impact_metrics:
      business_metrics:
      - "销售剧本与 AI 机会推荐采用率 82%，新增 ARR 贡献 2800 万人民币"
      - "区域 pipeline 可视化提升 35%，高层仪表盘满意度 NPS +40"
      technical_metrics:
      - "关键报表运行时间提升 2~3 倍，LLM 生成摘要延迟控制在 4 秒以内"
      - "数据准确率提升至 99.5%，支撑 AI 训练与多市场复制"
      operational_metrics:
      - "建立企业级数仓与实验手册并推广至其他市场"
      - "LLM 审核 + Prompt QA 流程将风险缺陷关闭时间缩短 45%"
    governance_artifacts:
    - exec_dashboard
    - risk_register
    - prompt_playbook
    - ai_policy
    project_overview: "为 Zoetis 全球 NGSE（Next Generation Sales Engine）在中国落地提供合规的 AI 引擎与湖仓底座，解决数据出入境限制、模型口径差异与推荐算法设计难题，让本地销售团队可复用全球剧本并量化商业化收益。"
    data_domain: "全球销售引擎 / 数据平台"
    ai_component_flag: true
    challenges_or_objectives:
      - "在中国数据进出口法规下，为全球 NGSE 构建可审计的本地数据管线与 AI 引擎"
      - "对齐 Zoetis 全球标准模型与国内经销/销售业务模型的字段、粒度与指标定义"
      - "验证国内销售/库存数据对 AI 训练、提示工程与推荐算法的适配性与质量"
      - "设计兼顾合规、可解释性的推荐算法与 Prompt 治理策略，支撑销售剧本复用"
    responsibilities:
      - "搭建 Inbox→Raw→Transform→Governed 四层湖仓架构，覆盖 CSV/JSON/Parquet/Avro 多格式落地、重放与审计，沉淀中国区镜像数据资产"
      - "拆解 ingestion/cleansing/join/aggregation 流程，利用 Databricks Workflows + global_temp/temp/intermediate 表和 Delta 版本回溯强化依赖管理与调试"
      - "手工注册 Delta 目录与 SQL Server 映射目录，统一元数据/权限策略，并落地 T-1 全量+增量差异校验机制"
      - "编写并调优 Spark SQL/PySpark 公共 ETL、加密/脱敏/路径解析/动态参数模块，结合参数化 Notebook（Widgets）加速多市场复制"
      - "整合 ADF、Azure Function、Databricks REST API 与 Aliyun OSS→ADLS Gen2 传输链路，实现跨平台调度、自动邮件/Teams 校验通知与业务触发作业"
      - "定义 Sales Copilot 推荐算法与 Prompt QA 流程，主持高层、风险与商业化复盘"
      - "组织国内市场数据采集、字段映射与批量校验，构建训练/测试/验证数据集供推荐算法与 AI 模型复盘"
    architecture_or_solution:
      - "四层数据处理架构：Inbox（多格式接入）→Raw（标准化/命名规范）→Transform（清洗/关联/聚合）→Governed（治理/血缘/权限/发布）"
      - "Databricks Workflows 拆分 ingestion/cleansing/join/aggregation 模块，配套 global_temp/temp/intermediate 宽表调试层"
      - "Delta Lake 版本与事务管理支撑批处理回溯、全量/增量切换与覆盖销售/产品流向宽表的 T-1 差异自动校验"
      - "元数据与方法管理：人工注册 Delta 目录 + SQL Server 映射、方法注册机制、Hive SQL 脚本自动加载"
      - "参数化 Notebook 模板 + Widgets + 公共 ETL（加密、脱敏、路径解析、动态参数）形成公司级数仓规范"
      - "Azure Function ↔ Databricks 调用环路：校验结果邮件/Teams 告警与业务触发作业"
      - "ADF 管理 Linked Services/Integration Runtime，负责 Aliyun OSS→ADLS Gen2 传输与 Databricks Notebook 定时调度"
      - "湖仓一体 + Auto Loader + 双通道消费架构，统一宽表供 AI 训练并满足 Power BI/Fabric 实时查询"
      - "Sales Copilot 推荐算法：LLM 摘要、下一步建议、风险提示、聊天助手嵌入 Power BI/CRM"
    process_or_methodology:
      - "湖仓一体 + PromptOps + 数据出入境合规三轨治理，输出授权与脱敏策略"
      - "模块化、原子化 Notebook/ETL/方法注册规范和 DNA 架构，支撑可插拔维度与事实表"
      - "自动化数据质量校验（Schema Drift、字段完整性、空值/重复）+ Azure Function 告警闭环"
      - "双周 OKR/路线图、推荐算法复盘与风险评审会，保障 exec/ops 共识"
    deliverables_or_features:
      - "Databricks 管道蓝图：ingestion/cleansing/join/aggregation 模块与 global_temp/temp/intermediate 表管理"
      - "元数据目录（Delta→SQL Server 映射）与方法注册脚本，含加密/脱敏/路径解析/动态参数模块"
      - "Schema Drift、字段完整性、空值/重复监测与自定义校验规则库"
      - "Azure Function 驱动的校验邮件/Teams 告警与 Databricks 作业触发闭环"
      - "Aliyun OSS→ADLS Gen2 跨平台传输流水线与 Databricks REST API 自动部署流程"
      - "Sales Copilot 推荐算法与双通道消费（AI 训练 + Power BI 实时查询）"
      - "训练/测试/验证数据集资产包与数据验证脚本，支撑模型回归与监管抽检"
    metrics_or_impact:
      - "关键报表（库存/销售）运行时间提升 2~3 倍，满足 AI 训练与 BI 查询 SLA"
      - "T-1 差异校验覆盖销售流向、产品流向等大表，数据准确率保持 99.5%"
      - "AI 推荐命中率 70%，销售人员每周节省 6 小时线索整理时间"
      - "建立公司级数仓 + Prompt 治理规范并在 4 个市场复用，风险事件零高危升级"
    tech_stack:
      - "Databricks"
      - "Delta Lake"
      - "Azure Data Factory"
      - "Azure Data Lake Storage Gen2"
      - "Aliyun OSS"
      - "Spark SQL"
      - "PySpark"
      - "Azure Functions"
      - "Power BI + Fabric"
      - "Azure OpenAI / GLM API"
      - "SQL Server"
    tools_platforms:
      - "Databricks Workflows"
      - "ADF"
      - "Azure Data Lake Storage Gen2"
      - "Azure Function Apps"
      - "Aliyun OSS"
      - "Prompt QA 工具链"
      - "Databricks REST API"
    team_info:
      team_size: 5
      description: "跨市场 PO + 数据工程 + 分析师混编小组"
    notes: "负责中国市场的路线图与 PromptOps 治理，成果被纳入全球销售数字化参考蓝本。"

  - project_name: "Remedium (BI)"
    company_or_context: "PWC"
    timeframe:
      label: "2016.10 - 2023.2"
      start: "2016-10"
      end: "2023-02"
    role_title: "高级技术顾问（解决方案架构）"
    role_perspective: architect
    llm_primary_role: data_development
    llm_secondary_roles:
    - full_stack
    management_scope:
      team_size: null
      budget_level: null
      stakeholder_tiers:
      - exec
      - director
      - ops
      - regulator
    decision_accountability:
    - delivery_owner
    - technical_strategy
    - risk_governance
    responsibility_focus:
    - architecture
    - stakeholder_management
    - compliance
    impact_metrics:
      business_metrics:
      - "客户监管审计全部一次通过，透明化报告缩短审批等待"
      technical_metrics:
      - "并行导入方案将 SAP/TPPT 批量处理窗口压缩到 SLA 以内"
      - "多层数据模型 + 质量校验让主数据准确率维持 99%+"
      operational_metrics:
      - "搭建 24 小时中欧 onshore 协作节奏并沉淀问题复盘机制"
    governance_artifacts:
    - training_plan
    - risk_register
    project_overview: "针对制药行业的 Remedium BI 平台，汇聚各国/各地区 MDM 数据，构建可追溯的 ETL + 数据模型骨干，并以 Web 门户向客户和审计方提供透明流程与报告。"
    data_domain: "制药行业 BI"
    ai_component_flag: false
    challenges_or_objectives:
      - "统一 SAP、TPPT 及调研来源的跨区域 MDM 数据，形成可扩展架构"
      - "解决多大文件导入的性能瓶颈并稳定 ADF + Databricks ETL 层"
      - "为业务与审计团队提供流程透明、可自助取数的门户"
    responsibilities:
      - "联合 onshore 团队完成需求澄清、解决方案评审与交付路线图"
      - "主导 ADF + Databricks 的 ETL 层设计，覆盖 SAP、TPPT 及研究渠道的标准化导入与监控"
      - "设计多线程/分片的数据接入策略，构建可配置导入工具与性能监控以支撑大文件批处理"
      - "依据客户业务架构交付可复用的数据模型与流程模板"
      - "规划并落地 Remedium Web 门户，支撑透明流程与客户报告"
    architecture_or_solution:
      - "分层数据模型：核心 MDM 骨干 + 客户特定扩展域"
      - "ADF Pipeline + Databricks 批处理框架支撑多线程/分片导入与计算"
      - "可配置 ETL 工具箱：源适配器、数据质量校验、性能监控脚本"
      - "Remedium Web Portal + 报告 API，贯通审计流程"
    process_or_methodology:
      - "需求→架构→模型评审工作坊，与 onshore 团队周度同步"
      - "数据标准化与流程模板治理，确保跨地区一致"
      - "性能回归与导入演练，结合问题复盘机制"
    deliverables_or_features:
      - "跨区域 MDM ETL 包与源适配器（SAP、TPPT、调研数据）"
      - "ADF/Databricks 批处理作业与开发效率工具"
      - "客户级数据模型蓝图 + 标准化业务流程包"
      - "Remedium Web 门户：透明流程追踪、审计报告生成"
    metrics_or_impact:
      - "稳定处理多地区 MDM 大文件并在 ADF/Databricks 上满足客户批处理 SLA"
      - "标准化数据模型缩短新客户上线周期并增强一致性"
      - "Web 门户让业务与审计团队实时共享透明流程，减少人工对接"
    tech_stack:
      - "Azure Data Factory"
      - "Databricks"
      - "Microsoft SQL Server"
      - "SAP / TPPT 数据源"
      - "MDM 平台"
      - ".NET / Web Portal 技术栈"
    tools_platforms:
      - "ADF Pipeline & Monitor"
      - "Databricks Workflows"
      - "SQL Server Agent"
      - "IIS / 内部 Web 门户"
      - "性能监控与日志工具"
    team_info:
      team_size: null
      description: "与欧洲 onshore 团队及本地交付团队共建"
    notes: "负责 Remedium 架构、ETL 与门户落地，聚焦制药合规场景的透明数据服务。"
